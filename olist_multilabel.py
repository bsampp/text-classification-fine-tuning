# -*- coding: utf-8 -*-
"""olist_multilabel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZfIWY15hLSsuCvMbzD1mNe9WHoahgjQ2
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install wandb

!wandb login

import os
os.environ["WANDB_PROJECT"]="olist-bertfinal"

!pip install torch
!pip install transformers[torch] datasets
!pip install scipy

from transformers import AutoModelForSequenceClassification, EarlyStoppingCallback
from datasets import load_dataset, ClassLabel

data_files = '/content/drive/My Drive/olist_order_reviews_dataset.csv'
ds = load_dataset('csv', data_files=data_files)

ds = ds.select_columns(['review_comment_message', 'review_score'])
ds = ds.rename_column('review_comment_message', 'text')
ds = ds.rename_column('review_score', 'label')

ds = ds.shuffle(seed=42)

# Filtrar linhas onde o texto ou o rating são nulos ou vazios
def filter_empty(example):
    return example['text'] is not None and example['text'].strip() != "" and example['label'] is not None

ds = ds.filter(filter_empty)

# Embaralhar o dataset
ds = ds.shuffle(seed=42)

# Dividir o dataset em 80% treino e 20% validação
split_ds = ds['train'].train_test_split(test_size=0.1, seed=42)

train_ds = split_ds['train']
test_ds = split_ds['test']

test_ds

train_ds = train_ds.filter(lambda example: example['text'] is not None and example['label'] is not None)
test_ds = test_ds.filter(lambda example: example['text'] is not None and example['label'] is not None)

# Contar amostras por classificação
label_distribution = {i: 0 for i in range(1, 6)}

for example in ds['train']:
    label_distribution[example['label']] += 1

print("Distribuição das classificações no dataset:")
for label, count in label_distribution.items():
    print(f"Classificação {label}: {count} amostras")

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)
#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)

def tokenize(examples):
    # Ensure the batch is a list of strings
    texts = examples['text']

    # Tokenize the texts
    encodings = tokenizer(texts, truncation=True, padding='max_length',max_length=128)

    # Cast labels to float (assuming 'label' is the key for your ratings)
    encodings['label'] = [label - 1 for label in examples['label']]

    return encodings

# Apply the tokenizer to the datasets
tok_train_ds = train_ds.map(tokenize, batched=True)
tok_test_ds = test_ds.map(tokenize, batched=True)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
def compute_metrics(p):
    # Extract the logits and the labels
    logits, labels = p
    predictions = np.argmax(logits, axis=-1)

    # Compute the accuracy
    accuracy = accuracy_score(labels, predictions)

    # Compute precision, recall, and F1 score
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
    }



import tempfile
from sklearn.model_selection import KFold
temp_output_dir = tempfile.mkdtemp()

from transformers import Trainer, TrainingArguments, DataCollatorWithPadding

batch_size = 64

num_folds = 5

kf = KFold(n_splits=num_folds)

accuracies = []
f1_scores = []
precision_scores = []
recall_scores = []

train_losses = []
val_losses = []

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

def get_training_args(fold_num):
    output_dir = os.path.join(checkpoint_dir, f"fold_{fold_num}")
    return TrainingArguments(
        output_dir=output_dir,
        #num_train_epochs=1,
        #per_device_train_batch_size=batch_size,
        #per_device_eval_batch_size=batch_size,
        #warmup_steps=500,
        #weight_decay=0.01,
        logging_steps=10,
        save_steps=5000,
        save_total_limit=3,
        load_best_model_at_end=True,
        eval_strategy="steps",
        eval_steps=5000,
        dataloader_num_workers=4,
        report_to="wandb",
        run_name=f"bertimbau-base-default-fold-{fold_num}"
    )

import numpy as np
import os
from time import time
import wandb
checkpoint_dir = "./model_checkpoints"

best_model_path = ""
best_val_loss = float('inf')

for fold_num, (train_index, val_index) in enumerate(kf.split(np.arange(len(tok_train_ds)))):
    fold_output_dir = os.path.join(checkpoint_dir, f"fold_{fold_num}")
    os.makedirs(fold_output_dir, exist_ok=True)

    model = AutoModelForSequenceClassification.from_pretrained("neuralmind/bert-base-portuguese-cased", num_labels=5)
    #model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
    train_fold = tok_train_ds.select(train_index)
    val_fold = tok_train_ds.select(val_index)

    trainer = Trainer(
        model=model,
        args=get_training_args(fold_num),
        train_dataset=train_fold,
        eval_dataset=val_fold,
        compute_metrics=compute_metrics,
        data_collator=data_collator,
    )

    print(f"Fold {fold_num + 1}")
    start_time = time()

    checkpoint_path = os.path.join(fold_output_dir, "checkpoint-xxxx")

    if os.path.exists(checkpoint_path):
        trainer.train(resume_from_checkpoint=checkpoint_path)
    else:
        print(f"Checkpoint not found at {checkpoint_path}. Starting training from scratch.")
        trainer.train()

    end_time = time()
    training_time = end_time - start_time
    wandb.log({"fold": fold_num, "training_time": training_time})
    print(f"Training time for fold {fold_num + 1}: {training_time} seconds")

    result = trainer.evaluate()

    accuracy = result["eval_accuracy"]
    f1 = result["eval_f1"]
    precision = result["eval_precision"]
    recall = result["eval_recall"]
    val_loss = result["eval_loss"]

    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1: {f1}")
    print(f"Val Loss: {val_loss}")

    accuracies.append(accuracy)
    f1_scores.append(f1)
    precision_scores.append(precision)
    recall_scores.append(recall)
    val_losses.append(val_loss)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_path = trainer.state.best_model_checkpoint

print(f"Best model path: {best_model_path}")

mean_accuracy = np.mean(accuracies)
mean_f1 = np.mean(f1_scores)
mean_precision = np.mean(precision_scores)
mean_recall = np.mean(recall_scores)
mean_val_loss = np.mean(val_losses)

print(f"Mean accuracy: {mean_accuracy}")
print(f"Mean F1 score: {mean_f1}")
print(f"Mean precision: {mean_precision}")
print(f"Mean recall: {mean_recall}")
print(f"Mean val loss: {mean_val_loss}")

# Carregando e avaliando o melhor modelo no conjunto de teste
best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)
trainer = Trainer(
    model=best_model,
    args=get_training_args(0),  # Os argumentos são irrelevantes para a avaliação
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

test_result = trainer.evaluate(eval_dataset=tok_test_ds)
test_accuracy = test_result["eval_accuracy"]
test_f1 = test_result["eval_f1"]
test_precision = test_result["eval_precision"]
test_recall = test_result["eval_recall"]

print(f"Test Accuracy: {test_accuracy}")
print(f"Test F1: {test_f1}")
print(f"Test Precision: {test_precision}")
print(f"Test Recall: {test_recall}")

import torch
input_text = "produto perfeito!"

inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
inputs = {k: v.to(device) for k, v in inputs.items()}

model.eval()

with torch.no_grad():
    outputs = model(**inputs)

logits = outputs.logits

probabilities = torch.nn.functional.softmax(logits, dim=-1)

predicted_class_idx = torch.argmax(probabilities, dim=1).item()

class_labels = ['1', '2', '3', '4', '5']

predicted_class_label = class_labels[predicted_class_idx]

print(f"Text: {input_text}")
print(f"Predicted Class: {predicted_class_label} (Index: {predicted_class_idx})")
print(f"Probabilities: {probabilities}")

wandb.finish()